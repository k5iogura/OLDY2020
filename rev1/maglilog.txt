ARGS data_path       = /Users/kenji/Downloads/work/OLDY2020/input_images
ARGS output_folder   = /Users/kenji/Downloads/work/OLDY2020/prediction
ARGS backbonename    = efficientnet-lite1
ARGS weights         = final.pth
ARGS image_size      = 256
Processor	= cpu
BlockArgs(num_repeat=1, kernel_size=3, stride=[1], expand_ratio=1, input_filters=32, output_filters=16, se_ratio=0.25, id_skip=True)
BlockArgs(num_repeat=2, kernel_size=3, stride=[2], expand_ratio=6, input_filters=16, output_filters=24, se_ratio=0.25, id_skip=True)
BlockArgs(num_repeat=2, kernel_size=5, stride=[2], expand_ratio=6, input_filters=24, output_filters=40, se_ratio=0.25, id_skip=True)
BlockArgs(num_repeat=3, kernel_size=3, stride=[2], expand_ratio=6, input_filters=40, output_filters=80, se_ratio=0.25, id_skip=True)
BlockArgs(num_repeat=3, kernel_size=5, stride=[1], expand_ratio=6, input_filters=80, output_filters=112, se_ratio=0.25, id_skip=True)
BlockArgs(num_repeat=4, kernel_size=5, stride=[2], expand_ratio=6, input_filters=112, output_filters=192, se_ratio=0.25, id_skip=True)
BlockArgs(num_repeat=1, kernel_size=3, stride=[1], expand_ratio=6, input_filters=192, output_filters=320, se_ratio=0.25, id_skip=True)
initialized model MalignancyDetector, with 5435004 parameters
<<< START EFFICIENTNET >>>
IN torch.Size([1, 3, 256, 256])
 ** Conv2dStaticSamePadding 256 128 3 32 (3, 3) (2, 2) (0, 1, 0, 1)
 ** BatchNorm2d 128 128 32 32 None None None
 ** ReLU6 128 128 32 32 None None None
 ** Conv2dStaticSamePadding 128 128 32 32 (3, 3) [1, 1] (1, 1, 1, 1)
 ** BatchNorm2d 128 128 32 32 None None None
 ** Conv2dStaticSamePadding 128 128 32 16 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 128 128 16 16 None None None
** no skipadd True [1] 32 16
 ** Conv2dStaticSamePadding 128 128 16 16 (3, 3) (1, 1) (1, 1, 1, 1)
 ** BatchNorm2d 128 128 16 16 None None None
 ** Conv2dStaticSamePadding 128 128 16 16 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 128 128 16 16 None None None
** skipadd output=output+input True 1 16 16
 ** Conv2dStaticSamePadding 128 128 16 96 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 128 128 96 96 None None None
 ** ReLU6 128 128 96 96 None None None
 ** Conv2dStaticSamePadding 128 64 96 96 (3, 3) [2, 2] (0, 1, 0, 1)
 ** BatchNorm2d 64 64 96 96 None None None
 ** ReLU6 64 64 96 96 None None None
 ** Conv2dStaticSamePadding 64 64 96 24 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 64 64 24 24 None None None
** no skipadd True [2] 16 24
 ** Conv2dStaticSamePadding 64 64 24 144 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 64 64 144 144 None None None
 ** ReLU6 64 64 144 144 None None None
 ** Conv2dStaticSamePadding 64 64 144 144 (3, 3) (1, 1) (1, 1, 1, 1)
 ** BatchNorm2d 64 64 144 144 None None None
 ** ReLU6 64 64 144 144 None None None
 ** Conv2dStaticSamePadding 64 64 144 24 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 64 64 24 24 None None None
** skipadd output=output+input True 1 24 24
 ** Conv2dStaticSamePadding 64 64 24 144 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 64 64 144 144 None None None
 ** ReLU6 64 64 144 144 None None None
 ** Conv2dStaticSamePadding 64 64 144 144 (3, 3) (1, 1) (1, 1, 1, 1)
 ** BatchNorm2d 64 64 144 144 None None None
 ** ReLU6 64 64 144 144 None None None
 ** Conv2dStaticSamePadding 64 64 144 24 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 64 64 24 24 None None None
** skipadd output=output+input True 1 24 24
 ** Conv2dStaticSamePadding 64 64 24 144 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 64 64 144 144 None None None
 ** ReLU6 64 64 144 144 None None None
 ** Conv2dStaticSamePadding 64 32 144 144 (5, 5) [2, 2] (1, 2, 1, 2)
 ** BatchNorm2d 32 32 144 144 None None None
 ** ReLU6 32 32 144 144 None None None
 ** Conv2dStaticSamePadding 32 32 144 40 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 32 32 40 40 None None None
** no skipadd True [2] 24 40
 ** Conv2dStaticSamePadding 32 32 40 240 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 32 32 240 240 None None None
 ** ReLU6 32 32 240 240 None None None
 ** Conv2dStaticSamePadding 32 32 240 240 (5, 5) (1, 1) (2, 2, 2, 2)
 ** BatchNorm2d 32 32 240 240 None None None
 ** ReLU6 32 32 240 240 None None None
 ** Conv2dStaticSamePadding 32 32 240 40 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 32 32 40 40 None None None
** skipadd output=output+input True 1 40 40
 ** Conv2dStaticSamePadding 32 32 40 240 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 32 32 240 240 None None None
 ** ReLU6 32 32 240 240 None None None
 ** Conv2dStaticSamePadding 32 32 240 240 (5, 5) (1, 1) (2, 2, 2, 2)
 ** BatchNorm2d 32 32 240 240 None None None
 ** ReLU6 32 32 240 240 None None None
 ** Conv2dStaticSamePadding 32 32 240 40 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 32 32 40 40 None None None
** skipadd output=output+input True 1 40 40
 ** Conv2dStaticSamePadding 32 32 40 240 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 32 32 240 240 None None None
 ** ReLU6 32 32 240 240 None None None
 ** Conv2dStaticSamePadding 32 16 240 240 (3, 3) [2, 2] (0, 1, 0, 1)
 ** BatchNorm2d 16 16 240 240 None None None
 ** ReLU6 16 16 240 240 None None None
 ** Conv2dStaticSamePadding 16 16 240 80 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 16 16 80 80 None None None
** no skipadd True [2] 40 80
 ** Conv2dStaticSamePadding 16 16 80 480 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 16 16 480 480 None None None
 ** ReLU6 16 16 480 480 None None None
 ** Conv2dStaticSamePadding 16 16 480 480 (3, 3) (1, 1) (1, 1, 1, 1)
 ** BatchNorm2d 16 16 480 480 None None None
 ** ReLU6 16 16 480 480 None None None
 ** Conv2dStaticSamePadding 16 16 480 80 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 16 16 80 80 None None None
** skipadd output=output+input True 1 80 80
 ** Conv2dStaticSamePadding 16 16 80 480 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 16 16 480 480 None None None
 ** ReLU6 16 16 480 480 None None None
 ** Conv2dStaticSamePadding 16 16 480 480 (3, 3) (1, 1) (1, 1, 1, 1)
 ** BatchNorm2d 16 16 480 480 None None None
 ** ReLU6 16 16 480 480 None None None
 ** Conv2dStaticSamePadding 16 16 480 80 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 16 16 80 80 None None None
** skipadd output=output+input True 1 80 80
 ** Conv2dStaticSamePadding 16 16 80 480 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 16 16 480 480 None None None
 ** ReLU6 16 16 480 480 None None None
 ** Conv2dStaticSamePadding 16 16 480 480 (3, 3) (1, 1) (1, 1, 1, 1)
 ** BatchNorm2d 16 16 480 480 None None None
 ** ReLU6 16 16 480 480 None None None
 ** Conv2dStaticSamePadding 16 16 480 80 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 16 16 80 80 None None None
** skipadd output=output+input True 1 80 80
 ** Conv2dStaticSamePadding 16 16 80 480 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 16 16 480 480 None None None
 ** ReLU6 16 16 480 480 None None None
 ** Conv2dStaticSamePadding 16 16 480 480 (5, 5) [1, 1] (2, 2, 2, 2)
 ** BatchNorm2d 16 16 480 480 None None None
 ** ReLU6 16 16 480 480 None None None
 ** Conv2dStaticSamePadding 16 16 480 112 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 16 16 112 112 None None None
** no skipadd True [1] 80 112
 ** Conv2dStaticSamePadding 16 16 112 672 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 16 16 672 672 None None None
 ** ReLU6 16 16 672 672 None None None
 ** Conv2dStaticSamePadding 16 16 672 672 (5, 5) (1, 1) (2, 2, 2, 2)
 ** BatchNorm2d 16 16 672 672 None None None
 ** ReLU6 16 16 672 672 None None None
 ** Conv2dStaticSamePadding 16 16 672 112 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 16 16 112 112 None None None
** skipadd output=output+input True 1 112 112
 ** Conv2dStaticSamePadding 16 16 112 672 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 16 16 672 672 None None None
 ** ReLU6 16 16 672 672 None None None
 ** Conv2dStaticSamePadding 16 16 672 672 (5, 5) (1, 1) (2, 2, 2, 2)
 ** BatchNorm2d 16 16 672 672 None None None
 ** ReLU6 16 16 672 672 None None None
 ** Conv2dStaticSamePadding 16 16 672 112 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 16 16 112 112 None None None
** skipadd output=output+input True 1 112 112
 ** Conv2dStaticSamePadding 16 16 112 672 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 16 16 672 672 None None None
 ** ReLU6 16 16 672 672 None None None
 ** Conv2dStaticSamePadding 16 16 672 672 (5, 5) (1, 1) (2, 2, 2, 2)
 ** BatchNorm2d 16 16 672 672 None None None
 ** ReLU6 16 16 672 672 None None None
 ** Conv2dStaticSamePadding 16 16 672 112 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 16 16 112 112 None None None
** skipadd output=output+input True 1 112 112
 ** Conv2dStaticSamePadding 16 16 112 672 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 16 16 672 672 None None None
 ** ReLU6 16 16 672 672 None None None
 ** Conv2dStaticSamePadding 16 8 672 672 (5, 5) [2, 2] (2, 2, 2, 2)
 ** BatchNorm2d 8 8 672 672 None None None
 ** ReLU6 8 8 672 672 None None None
 ** Conv2dStaticSamePadding 8 8 672 192 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 8 8 192 192 None None None
** no skipadd True [2] 112 192
GO-0 torch.Size([1, 24, 64, 64]) GO-1 torch.Size([1, 40, 32, 32]) GO-2 torch.Size([1, 80, 16, 16]) GO-3 torch.Size([1, 192, 8, 8]) 
>>> ENDED EFFICIENTNET <<<

 ** Conv2d 8 8 192 96 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 8 8 96 96 None None None
 ** ReLU6 8 8 96 96 None None None
<<< START R_ASPP >>>
IN-1 torch.Size([1, 96, 8, 8]) IN-2 torch.Size([1, 40, 32, 32])
<<< START ROUTE-1-1 >>>
IN torch.Size([1, 96, 8, 8])
 ** Conv2d 8 8 96 128 (1, 1) (1, 1) (0, 0)
 ** BatchNorm2d 8 8 128 128 None None None
GO torch.Size([1, 128, 8, 8])
<<< ENDED ROUTE-1-1 >>>

<<< START ROUTE-1-2 >>>
IN torch.Size([1, 96, 8, 8])
 ** AdaptiveAvgPool2d 8 1 96 96 None None None
 ** Conv2d 1 1 96 128 (1, 1) (1, 1) (0, 0)
 ** Hardsigmoid 1 1 128 128 None None None
 *** interpolate torch.Size([1, 128, 8, 8]) torch.Size([8, 8]) bilinear False
GO torch.Size([1, 128, 8, 8])
<<< ENDED ROUTE-1-2 >>>

<<< START ROUTE-1-AGRIGATE >>>
IN-1 torch.Size([1, 128, 8, 8]) IN-2 torch.Size([1, 128, 8, 8])
PRODUCT-GO torch.Size([1, 128, 8, 8])
torch.Size([1, 128, 8, 8])
 *** interpolate torch.Size([1, 128, 32, 32]) torch.Size([32, 32]) bilinear False
torch.Size([1, 128, 32, 32])
 ** Conv2d 32 32 128 19 (1, 1) (1, 1) (0, 0)
GO torch.Size([1, 19, 32, 32])
<<< ENDED ROUTE-1-AGRIGATE >>>

<<< START ROUTE-2 >>>
IN torch.Size([1, 40, 32, 32])
 ** Conv2d 32 32 40 19 (1, 1) (1, 1) (0, 0)
GO torch.Size([1, 19, 32, 32])
<<< ENDED ROUTE-2 >>>

<<< START ROUTE-AGRIGATE >>>
IN-1 torch.Size([1, 19, 32, 32]) IN-2 torch.Size([1, 19, 32, 32])
 *** torch.cat torch.Size([1, 19, 32, 32]) torch.Size([1, 19, 32, 32]) torch.Size([1, 38, 32, 32])
GO torch.Size([1, 38, 32, 32])
<<< ENDED ROUTE-AGRIGATE >>>

GO torch.Size([1, 38, 32, 32])
<<< ENDED R_ASPP >>>

 ** Dropout2d 32 32 38 38 None None None
 ** Conv2d 32 32 38 2 (1, 1) (1, 1) (0, 0)
 *** interpolate torch.Size([1, 2, 32, 32]) torch.Size([256, 256]) bilinear True
Total inference time:  0.12125492095947266  Secondes
